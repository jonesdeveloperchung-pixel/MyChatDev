# USAGE Guide for Cooperative LLM System

This guide provides detailed instructions on how to set up, configure, run, and interpret the outputs of the Cooperative LLM System.

## ðŸš€ Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/MyChatDev.git
    cd MyChatDev
    ```

2.  **Install Python dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Ensure Ollama is running:**
    The system relies on Ollama for local LLM inference. Make sure your Ollama server is running.
    ```bash
    ollama serve
    ```

4.  **Verify required models are available:**
    Check if the models specified in your chosen LLM profile (e.g., `high_reasoning`, `low_reasoning`) are downloaded and available in Ollama.
    ```bash
    ollama list
    ```
    If a model is missing, download it:
    ```bash
    ollama pull model-name:tag
    ```

## ðŸ”§ Configuration

The system's behavior can be configured in two primary ways:

1.  **`config/settings.py`:** This file defines the default system-wide parameters (`SystemConfig`) and the structure for LLM configurations (`LLMConfig`).
2.  **`config/llm_profiles.py`:** This file defines different LLM profiles, each mapping agent roles to specific LLM models, temperatures, and other parameters.

### LLM Model Assignment

You can customize which LLM models are used for each agent role by editing `config/llm_profiles.py`. Different profiles (e.g., `high_reasoning`, `low_reasoning`) are provided to suit various needs for speed and reasoning capability.

### System Parameters

System-wide parameters like `max_iterations`, `quality_threshold`, `enable_sandbox`, etc., are defined in `config/settings.py`.

## ðŸŽ¯ Running the System

The primary way to interact with the system is through the `cli.py` script.

### Basic Execution

To run the system with default settings and a predefined prompt:

```bash
python cli.py
```

### Running via CLI with Custom Parameters

...

### Understanding Prompt Flow

It's important to understand how prompts are utilized across different LLM roles within the system:

*   **Initial User Prompt (`-U` parameter):** The content provided via the `-U` parameter (or the built-in default prompt) serves as the primary input for the **Product Manager** role. This role is responsible for analyzing and refining these initial requirements.
*   **Subsequent Role Inputs:** For all other LLM roles (e.g., Architect, Programmer, Tester, Reviewer, Reflector), their main input is typically the *deliverables* or outputs generated by preceding roles in the workflow. They do not directly receive the original user prompt. This iterative process ensures that each role builds upon the work of the previous one, refining the solution progressively.

## ðŸ“¦ Interpreting Deliverables

Upon completion, the system generates a timestamped folder in the `deliverables/` directory (e.g., `deliverables/20251021-144343`). This folder contains:

*   **`requirements_specification.md`**: The detailed requirements analyzed by the Product Manager.
*   **`system_design.md`**: The architectural design created by the System Architect.
*   **`source_code.py`**: The implemented code from the Programmer agent.
*   **`test_results.md`**: The results of test generation and execution by the Tester agent.
*   **`review_feedback.md`**: Feedback from the Code Reviewer.
*   **`strategic_guidance.md`**: Guidance from the Reflector agent (if triggered).
*   **`complete_state.json`**: A JSON file containing the full workflow state, including iteration counts, quality evaluations, and final decisions.

## ðŸ“ˆ Performance Analysis

The `main.py` script includes a framework for running multiple test cases and collecting performance data. After running `main.py`, a `performance_summary_YYYYMMDD_HHMMSS.json` file will be generated in the `deliverables/` folder, summarizing key metrics for each test case.

Key metrics to analyze include:
*   **Success Rate**: Percentage of prompts leading to a successful halt.
*   **Iteration Count**: Efficiency of the workflow.
*   **Code Quality**: Final `overall_quality_score` from the Quality Gate.
*   **Time to Completion**: Total execution time.
*   **Stagnation Rate**: How often the Reflector is triggered.

## ðŸš¨ Troubleshooting

Refer to the "Troubleshooting" section in `README.md` for common issues and solutions.

### Common Issues (Specific to Quality Gate LLM)

1. **Quality Gate LLM Not Returning Valid JSON**
   - This can manifest as `LLM assessment is not valid JSON` warnings and the workflow getting stuck in a loop.
   - **Cause:** The LLM assigned to the Quality Gate role is not consistently adhering to the strict JSON output format specified in its prompts.
   - **Solution:**
     - **Model Selection:** Ensure you are using a sufficiently capable LLM for the Quality Gate role (e.g., `gemma3:4b` or larger models are generally better at strict instruction following than `tinyllama`).
     - **Prompt Review:** Carefully review `config/system_quality_gate_prompt.txt` and `config/quality_gate_prompt.txt` to ensure the instructions for JSON output are clear, unambiguous, and strongly emphasized.
     - **Parsing Robustness:** The system includes robust parsing logic, but consistent JSON output from the LLM is paramount.

---